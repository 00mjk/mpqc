\section{A First Pass}

Need to distinguish between problems and methods.
The general optimization problem is :
\begin{eqnarray}
\min_{x \in R^{n}} 	& f(x) 	& \\
\mbox{subject to} 		& h_i(x) = 0, & \quad i = 1, \ldots, m \nonumber\\
	 		& g_i(x) \leq 0, & \quad i = 1, \ldots, p \nonumber 
\end{eqnarray}
Let's consider the unconstrained optimization problem first. Most
methods used will assume certain properties for the function $f(x)$.
For example, one or more of the following:
\begin{itemize}
\item How smooth is the function ? Differentiability, C0, C1, C2, ...
\item Are there any special properties
\begin{itemize}
\item Linear function, quadratic function, etc.
\item Large scale, partially separable
\item Noisy
\end{itemize}
\item Is there any special structure to the problem
\end{itemize}

To consider the first property, smoothness, available algorithms can
be classified according to the amount of smoothness assumed in the
function. For example, if the function is $C^2$ (twice continuously
differentiable), then one could use a Newton method. On the other end
of the scale, if the function is only $C^0$, then one could use a
direct-search method.

It seems appropriate to distinguish between optimization problems and optimization algorithms.


One of the first questions that arises is the degree of continuity available.
This fact may not be readily available, but what is clear is the availability of analytic derivatives.  As such we've chosen to classify nonlinear programming problems by the availability of functions for computing the derivatives.

\begin{itemize}
\item No derivative information available
\item Analytic first derivatives are available
\item Analytic first and second derivatives are available
\end{itemize}

In terms of optimization algorithms, we've chosen to classify methods
into three very broad classes:

\begin{itemize}
\item Direct Search methods
\item Conjugate gradient methods
\item Newton type methods
\end{itemize}

These divisions are rather arbitrary, but many well-known methods fall
into these three categories.  For example, methods such as the
Nelder-Mead simplex method, the box method, and pattern methods fall
into the direct search class. The nonlinear conjugate gradient method
and limited memory BFGS methods fall into the Conjugate Gradient
class. Finally, any variation on the Newton method would fall into the
Newton class, including finite-difference Newton, quasi-Newton
methods, and inexact Newton methods.
